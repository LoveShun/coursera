\section{线性回归(Linear Regression)}
\subsection{各向量形式}
在机器学习中，各个变量在单独出现时均为列向量的形式，但若是以多个向量形成的矩阵形式出现时，均为行向量的形式。\\
以下将会写出各个变量单独出现的情况、各变量以矩阵的形式一起出现的情况。
\begin{enumerate}
\item $\vec{x}$
\begin{equation}
	\vec{x} = \left(\begin{matrix}
				x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
			\end{matrix}\right)
			= \left(\begin{matrix}
				1 \\ x_1 \\ x_2 \\ \vdots \\ x_n
			\end{matrix}\right)_{(n+1)*1}
\end{equation}

\item $\vec{\theta}$
\begin{equation}
	\vec{\theta} = \left(\begin{matrix}
					\theta_0 \\ \theta_1 \\ \vdots \\ \theta_n
				\end{matrix}\right)_{(n+1)*1}
\end{equation}

\item $\vec{y}$
\begin{equation}
	\vec{y} = \left(\begin{matrix}
				y
			\end{matrix}\right)_{1*1}
\end{equation}

% \item $h_{\vec{\theta}}(\vec{x})$
% \begin{equation}\begin{aligned}
% 	h_{\vec{\theta}}(\vec{x}) & = \vec{\theta}^{T}\vec{X} = \vec{X}^T\vec{\theta} \\
% 		& = \left( \begin{matrix}
% 			1 & x_1 & x_2 & \dots & x_n
% 			\end{matrix}\right)
% 			\left(\begin{matrix}
% 				\theta_0 \\
% 				\theta_1 \\
% 				\dots \\
% 				\theta_n
% 			\end{matrix}\right) \\
% 		& = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n
% \end{aligned} \end{equation}
\end{enumerate}

\subsection{批梯度下降}
\subsubsection{各矩阵形式}
下述式子中均为矩阵的形式 \\
\begin{enumerate}
\item $X$
\begin{equation} \begin{aligned}
	X & = \left(\begin{matrix}
			\vec{x}^{(1)} \\ \vec{x}^{(2)} \\ \vec{x}^{(3)} \\ \vdots \\ \vec{x}^{(m)}
		\end{matrix}\right) \\
	& = \left( \begin{matrix}
			x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \dots & x_n^{(1)} \\
			x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \dots & x_n^{(2)} \\
			x_0^{(3)} & x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \dots & x_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & \dots & x_n^{(m)} \\
			\end{matrix}\right) \\
	& = \left(\begin{matrix}
			1 & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \dots & x_n^{(1)} \\
			1 & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \dots & x_n^{(2)} \\
			1 & x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \dots & x_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			1 & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & \dots & x_n^{(m)} \\
		\end{matrix}\right)_{m*(n+1)}
\end{aligned} \end{equation}

\item $\vec{\theta}$
\begin{equation} \begin{aligned}
	\vec{\theta} & = \left(\begin{matrix}
			\theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3 \\ \vdots \\ \theta_n \\
		\end{matrix}\right)_{(n+1)*1}
\end{aligned}\end{equation}

\item $h_{\vec{\theta}}(\vec{x})$
\begin{equation}\begin{aligned}
	h_{\vec{\theta}}(\vec{x}) &= X * \vec{\theta} \\
	    & = \left(\begin{matrix}
			1 & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \dots & x_n^{(1)} \\
			1 & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \dots & x_n^{(2)} \\
			1 & x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \dots & x_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			1 & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & \dots & x_n^{(m)} \\
		\end{matrix}\right)
		\left(\begin{matrix}
			\theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3 \\ \vdots \\ \theta_n \\
		\end{matrix}\right) \\
		& = \left(\begin{matrix}
			1\theta_0 + x_1^{(1)}\theta_1 + x_2^{(1)}\theta_2 + x_3^{(1)}\theta_3 + \dots + x_n^{(1)}\theta_n \\
			1\theta_0 + x_1^{(2)}\theta_1 + x_2^{(2)}\theta_2 + x_3^{(2)}\theta_3 + \dots + x_n^{(2)}\theta_n \\
			1\theta_0 + x_1^{(3)}\theta_1 + x_2^{(3)}\theta_2 + x_3^{(3)}\theta_3 + \dots + x_n^{(3)}\theta_n \\
			\vdots \\
			1\theta_0 + x_1^{(m)}\theta_1 + x_2^{(m)}\theta_2 + x_3^{(m)}\theta_3 + \dots + x_n^{(m)}\theta_n \\
		\end{matrix}\right)_{m*1}
\end{aligned}\end{equation}

\item $\vec{y}$
\begin{equation}
	\vec{y} = \left(\begin{matrix}
		y^{(1)} \\ y^{(2)} \\ y^{(3)} \\ \vdots \\ y^{(m)}
	\end{matrix}\right)_{m*1}
\end{equation}
\end{enumerate}

\subsubsection{Cost Function}
\begin{enumerate}
\item 数值计算形式：
\begin{equation}
	J(\theta) = \frac{1}{2m} \sum_{i=1}^m \left[ h_\theta(x^{(i)}) - y^{(i)}\right]^2
\end{equation}

\item 矩阵计算形式：
\begin{equation}
	J(\theta) = \frac{1}{2m} \left[h_{\vec{\theta}}(\vec{x}) - \vec{y}\right]^T \left[ h_{\vec{\theta}}(\vec{x}) - \vec{y}\right]
\end{equation}
\end{enumerate}


\subsubsection{偏导数$\frac{\partial J(\theta)}{\partial \theta_j}$计算}
\begin{enumerate}
\item 数值计算形式
\begin{equation}
	\frac{\partial J(\theta)}{\partial \theta_j} =
	    \frac{1}{m} \left[ h_\theta(x^{(i)}) - y^{(i)} \right] x_j^{(i)}
\end{equation}

\item 矩阵计算形式
\begin{equation}
	\nabla J(\theta) = \frac{1}{m} X^T \left[h_{\vec{\theta}}(\vec{x}) - \vec{y}\right]
\end{equation}
\end{enumerate}


\subsubsection{梯度下降迭代方式}
\begin{enumerate}
\item 数值计算形式
\begin{equation}\begin{aligned}
	\theta_j &:= \theta_j - \alpha\frac{\partial J(\theta)}{\partial \theta_j} \\
		&:= \theta_j - \alpha \frac{1}{m} \left[h_\theta(x^{(i)}) - y^{(i)}\right] x_j^{(i)} \\
\end{aligned}\end{equation}

\item 矩阵计算形式
\begin{equation}\begin{aligned}
	\theta &:= \theta - \alpha\nabla J(\theta) \\
		&:= \theta - \alpha \frac{1}{m} X^T \left[ h_{\vec{\theta}}(\vec{x}) - \vec{y}\right]
\end{aligned}\end{equation}
\end{enumerate}



\subsection{Feature Normalization}
\begin{equation}
	x_i = \frac{x_i - \mu}{\sigma}
\end{equation}
或
\begin{equation}
	x_i = \frac{x_i - \mu}{max - min}
\end{equation}



\subsection{公式法求解（Normal Equation）}
\begin{equation}
	\theta = (X^T X)^{-1} X^T y
\end{equation}















