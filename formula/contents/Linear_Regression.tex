\section{线性回归(Linear Regression)}
\subsection{当训练集X只有1项时}

\begin{equation}
	X = \left(\begin{matrix}
			x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
		\end{matrix}\right)
	= \left(\begin{matrix}
			1 \\ x_1 \\ x_2 \\ \vdots \\ x_n
		\end{matrix}\right)
\end{equation}


\begin{equation}
	\theta = \left(\begin{matrix}
			\theta_0 \\ \theta_1 \\ \vdots \\ \theta_n
		\end{matrix}\right)
\end{equation}


\begin{equation}
y = y
\end{equation}


\begin{equation} \begin{aligned}
	h_\theta(x) & = \theta^{T}X = X^T\theta \\
	& = \left( \begin{matrix}
			1 & x_1 & x_2 & \dots & x_n
		\end{matrix}\right)
		\left(\begin{matrix}
			\theta_0 \\
			\theta_1 \\
			\dots \\
			\theta_n
		\end{matrix}\right) \\
	& = \theta_0 + \theta_1x_1 + \theta_2x_2 + \dots + \theta_nx_n
\end{aligned} \end{equation}




\subsection{当训练集X有m项时}
\begin{equation} \begin{aligned}
	X & = \left(\begin{matrix}
			x^{(0)} \\ x^{(1)} \\ x^{(2)} \\ x^{(3)} \\ \vdots \\ x^{(m)} \\
		\end{matrix}\right) \\
	& = \left( \begin{matrix}
			x_0^{(1)} & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \dots & x_n^{(1)} \\
			x_0^{(2)} & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \dots & x_n^{(2)} \\
			x_0^{(3)} & x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \dots & x_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			x_0^{(m)} & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & \dots & x_n^{(m)} \\
			\end{matrix}\right) \\
	& = \left(\begin{matrix}
			1 & x_1^{(1)} & x_2^{(1)} & x_3^{(1)} & \dots & x_n^{(1)} \\
			1 & x_1^{(2)} & x_2^{(2)} & x_3^{(2)} & \dots & x_n^{(2)} \\
			1 & x_1^{(3)} & x_2^{(3)} & x_3^{(3)} & \dots & x_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			1 & x_1^{(m)} & x_2^{(m)} & x_3^{(m)} & \dots & x_n^{(m)} \\
		\end{matrix}\right)
\end{aligned} \end{equation}


\begin{equation} \begin{aligned}
	\theta & = \left(\begin{matrix}
			\theta^{(0)} \\ \theta^{(1)} \\ \theta^{(2)} \\ \theta^{(3)} \\ \vdots \\ \theta^{(m)} \\
		\end{matrix}\right) \\
	& = \left(\begin{matrix}
			\theta_0^{(0)} & \theta_1^{(0)} & \theta_2^{(0)} & \theta_3^{(0)} & \dots & \theta_n^{(0)} \\
			\theta_0^{(1)} & \theta_1^{(1)} & \theta_2^{(1)} & \theta_3^{(1)} & \dots & \theta_n^{(1)} \\
			\theta_0^{(2)} & \theta_1^{(2)} & \theta_2^{(2)} & \theta_3^{(2)} & \dots & \theta_n^{(2)} \\
			\theta_0^{(3)} & \theta_1^{(3)} & \theta_2^{(3)} & \theta_3^{(3)} & \dots & \theta_n^{(3)} \\
			\vdots    & \vdots    & \vdots    & \vdots    & \ddots & \vdots   \\
			\theta_0^{(m)} & \theta_1^{(m)} & \theta_2^{(m)} & \theta_3^{(m)} & \dots & \theta_n^{(m)} \\
			\end{matrix}\right)
\end{aligned}\end{equation}

Cost Function
\begin{enumerate}
\item 数值形式：
\begin{equation}
	J(\theta) = \frac{1}{2m} \left[ h_\theta(x^{(i)}) - y^{(i)}\right]^2
\end{equation}

\item 矩阵形式：
\begin{equation}
	J(\theta) = \frac{1}{2m} \left[h_\theta(x) - y\right]^T \left[ h_\theta(x) - y\right]
\end{equation}
\end{enumerate}

梯度下降
\begin{enumerate}
\item 数值形式
\begin{equation}
	\frac{\partial J(\theta)}{\partial \theta_j} =
	    \frac{1}{m} \left[ h_\theta(x^{(i)}) - y^{(i)} \right] x_j^{(i)}
\end{equation}

迭代方式：
\begin{equation}
	\theta_j := \theta_j - \alpha \frac{1}{m} \left[h_\theta(x^{(i)}) - y^{(i)}\right] x_j^{(i)}
\end{equation}

\item 矩阵形式
\begin{equation}
	\nabla J(\theta) = \frac{1}{2m} X^T \left[h_\theta(x) - y\right]
\end{equation}

迭代方式：
\begin{equation}
	\theta := \theta - \alpha \frac{1}{m} X^T \left[ h_\theta(x) - y\right]
\end{equation}
\end{enumerate}



\subsection{Feature Normalization}
\begin{equation}
	x_i = \frac{x_i - \mu}{\sigma}
\end{equation}
或
\begin{equation}
	x_i = \frac{x_i - \mu}{max - min}
\end{equation}



\subsection{公式法求解（Normal Equation）}
\begin{equation}
	\theta = (X^T X)^{-1} X^T y
\end{equation}















